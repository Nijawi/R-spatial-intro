---
title: "An Introduction to R Spatial"
format: html
#project:
 # type: website
  #output-dir: docs
theme: sandstone
author-title: Made for
author: Research Bazaar Queensland
toc: true
toc-location: left
toc-title: Content
toc-depth: 4
published-title: Date
date: 2025-07-03
editor: visual
embed-resources: true
---

### Using R for mapping and spatial data science

#### Why?

-   Reproducibility
-   Collaboration
-   Flexibility and Custom Exports
-   Automation and Data Pipelines
-   Big data

#### What you'll learn in these notes

1.  Overview of what spatial data is, with links to more in-depth resources
2.  Overview of R packages available for spatial data
3.  How to start a spatial project in R
4.  How to wrangle spatial data in R
5.  How to perform spatial operations in R
6.  How to make a map in R
7.  Common problems you might encounter in R spatial, and how to solve them

#### Things you'll need

These notes don't assume any prior knowledge of using R for spatial data, but do assume a basic understanding of how to use R.

Install [R](https://www.r-project.org/) and [RStudio](https://www.rstudio.com/products/rstudio/download/)

Install the following packages:

`install.packages('sf', 'tidyverse', 'tmap', 'dplyr', 'readr', 'ggplot2', 'forcats', 'units', 'Polychrome')`


**A quick note on mastering Rspatial**

A good way to start getting used to a new R package is to read any [vignettes](https://r-pkgs.org/vignettes.html) that are available, and to read the documentation associated with functions that you'd like to use.

Just use the '?' function to get the documentation. As an example, let's ask for help with the base R function we used above: 'install.packages'.

```{r}
?install.packages
```

The help documentation will tell you what a function can be used for, what arguments it takes, and the examples at the bottom of the documentation page can be **really** helpful for understanding how to use it. I've found the `terra` documentation to be particularly good for this (but the quality of documentation varies depending on package developers who write them).

### Brief overview of spatial data

What is spatial data? Most broadly-speaking, data is spatial when it has coordinates, and therefore a coordinate reference system (see more on this below). Coordinates define the longitude (x) and latitude (y) of where data is located across the globe.

#### Spatial data types

There are three broad classes of spatial data that you may encounter.

**1. Vector**

-   Vector data can be points, lines, or polygons. Quite typically vector data is stored as shapefiles (.shp), although there are many file formats to store vector data. My go-to is geopackages (.gpkg) (more on this later).

-   Often, vector data have multiple attributes, meaning that different variables have been measured for each vector geometry.

**2. Raster**

-   Raster data is gridded data, where each 'grid cell' (also referred to as a 'pixel') stores a data value. Data can be categorical (e.g. land use classes) or continuous (e.g. temperature).

-   Grids can be regular, irregular, curvilinear, etc.

**3. Spatiotemporal data cubes**

-   Spatiotemporal data cubes are when vector or raster data have an additional dimension beyond x & y coordinates: time.

**Switching between data types**

In some cases you might turn vector data into raster and vice-versa. R makes this relatively easy.

Often you'll want to use vector and raster data together, e.g., calculating average temperature at a study location. R makes this easy too. We'll do more of this later.

#### Coordinate reference systems

Coordinate reference systems are at the heart of how we make data spatial. They tell us how the coordinates of our data *refer* to, a.k.a *map* to, the earth. There are two main types: Geographic Coordinate reference Systems (GCS) and Projected Coordinate reference Systems (PCS).

![Figure1.](images/Fig2.png)

*Note* This image is borrowed from [here](https://www.esri.com/arcgis-blog/products/arcgis-pro/mapping/gcs_vs_pcs/#:~:text=What%20is%20the%20difference%20between,map%20or%20a%20computer%20screen.)

**1. Geographic coordinate reference systems (GCS)**

-   Every spatial dataframe has a GCS
-   It refers to a 3-dimensional representation of the Earth
-   X and Y coordinates are defined in *angular* lat-long units, e.g., decimal degrees
-   WGS84 is a common one you're probably familiar with

**2. Projected coordinate reference systems (PCS)**

-   To map spatial data in 2 dimensions (i.e., on a flat surface), we need to transform it (i.e., *project* it) to *linear* units (e.g., metres)
-   The projection of angular units to linear will result in some distortion of our spatial data
-   You can choose the best projected coordinate reference system depending on:
    a.  What characteristic of the spatial data you are most interested in preserving, e.g., area, distance, etc
    b.  Where you are in the world; there are global, national and local coordinate reference systems that will be most relevant depending on the scale and location of your data
    c.  There are resources out there to help you do this, including the [\`crsuggest' R package](https://github.com/walkerke/crsuggest)

But how come we see 'flat' maps that are in WGS84, a lat-long GCS? When we see a GCS mapped in 2 dimensions, it's using a *'pseudo-plate caree'* projection.

There is ALOT to know about coordinate reference systems. We don't have enough time to go through all of the details, but I recommend checking out this more comprehensive [introduction](https://www.esri.com/arcgis-blog/products/arcgis-pro/mapping/gcs_vs_pcs/#:~:text=What%20is%20the%20difference%20between,map%20or%20a%20computer%20screen.).

### R packages available for spatial data

![Figure 2.](images/Fig1.png)

*Note* Figure 1.2 is taken from [Geocomputation with R](https://geocompr.robinlovelace.net/intro.html)

Packages for spatial data have been around since the release of R in the year 2000. Since then, new packages are being developed all the time that supersede old ones. We'll use the most recent and *state-of-the-art* ones here. Namely, we'll use or discuss the following:

-   [sf](https://r-spatial.github.io/sf/articles/sf1.html)
    -   `sf` stands for 'simple features'
    -   it has largely replaced `sp`
    -   this is my go-to for vector data - points, polygons, lines
-   [terra](https://cran.r-project.org/web/packages/terra/index.html)
    -   `terra` has replaced `raster` for working with raster data
    -   it can also handle vector data (has it's own class, 'SpatVectors')
    -   it's super fast compared to `raster`
-   [stars](https://r-spatial.github.io/stars/)
    -   `stars` can handle spatiotemporal data cubes (both raster and vector)
    -   it can also handle irregular grids, which `terra` cannot
    -   still learning how to use it, some spatial data scientists I know do all of there vector and raster processing in `stars`. Others switch between packages depending on needs.

### How to start a spatial project in R

#### My basic set-up

1.  Make an analysis folder on cloud storage of choice (e.g., dropbox). Call it something meaningful, e.g., 'echidna-analysis'
2.  Add 3 folders nested inside the analysis folder: 'scripts', 'data', 'outputs'
3.  Open Rstudio, File -\> New Project -\> Existing Directory -\> browse for analysis folder created in step 1 and open
4.  Move spatial data files into the nested 'data' folder created in Step 2
5.  Open the project in Rstudio by double-clicking the .Rproj file
6.  Open a script, and you're good to start coding and analysing. You don't need to worry about setting the working directory. To read in data, you'll need to have 'data/...' at the start of your pathfile
7.  Make sure you save scripts in the nested 'scripts' folder
8.  Save outputs with the pathfile 'outputs/...'

When you're all done, your 'bird-analysis' folder should look like this inside.

![Figure 3.](images/proj.png)

I like this set-up because it keeps scripts, data and outputs nicely organised, and I can easily send the analysis folder to a collaborator and they'll be able to get started quickly by double-clicking the .Rproj file.

But this is just my highly opinionated set-up - ultimately, do what works for you!

#### Tips and Tricks

-   Number your scripts in the order you need them, e.g., 001_wrangle-data.R, 002_analyse-data.R
-   Make as many nested 'outputs' folders as you need, e.g., 'tables', 'figures', etc
-   Short scripts are better than long ones. I try to keep scripts bound to a unique task, e.g., wrangling vs. modelling
-   Use Jenny Bryan's [naming things](https://docplayer.net/55248970-Naming-things-prepared-by-jenny-bryan-for-reproducible-science-workshop.html)
-   Use github for version control and if the project is highly collaborative. See our lab's framework developed by the amazing Max Campbell [here](https://github.com/seascape-models/seascape_collaboration)

### How to wrangle and map spatial data in R

The first step of any data wrangling or analysis is usually getting the data into R. The way we read in spatial data will depend on the type of spatial data (vector vs. raster) we're using, and therefore the R package.

If you've used R before for wrangling non-spatial data, such as with the packages `dplyr` and `tidyr`, you're in luck - we can use them to wrangle our spatial data too!

Our spatial data consists of echidna sightings from [ALA](https://bie.ala.org.au/species/https://biodiversity.org.au/afd/taxa/0d4c9c0c-51d3-44e0-a365-fe0f8b791c66), electorate data from [the AEC](https://www.aec.gov.au/electorates/gis/gis_datadownload.htm), and broad vegetation groups from [QSpatial](https://qldspatial.information.qld.gov.au/catalogue/custom/detail.page?fid={43A2CB31-9D83-4BB9-ACE7-05E7BD271FE3}). What type of spatial data do you suspect these will be (vector or raster)?

#### Using `sf` to read in vector data

First we need to load the package using the `library` function. You can think of packages like books in a library, and loading a package is like taking one off the shelf.

```{r}
#| echo: true
#| output: false
library(sf)
```

Read in the electorate boundaries.

```{r}
elec <- st_read('data/E_AUGEC_region.shp')
```

You should now see an object in your RStudio environment called 'elec'. This is a simple features dataframe, which is essentially a dataframe with a geometry column (storing the coordinates for each observation, in this case each observation is a federal electorate in QLD).

The output in the console gives us some extra information about 'elec'. It has 30 features (rows/sites) and 10 fields (attributes/variables).

The metadata also tells us that the geometry type is 'multipolygon', there are two dimensions (x, and y), the bounding box (min and max x and y coordinates of all the spatial data points), and the projected coordinate reference system (CRS).

Before we start wrangling, we might like to get even more familiar with the structure of the spatial data. Here are a few handy functions for doing that.

Check the class of the R object. Confirm it is a simple feature (sf) dataframe.

```{r}
class(elec)
```

Check the structure of the spatial dataframe.

```{r}
str(elec)
```

See the first six rows.

```{r}
head(elec)
```

Get the column names.

```{r}
colnames(elec)
```

**A brief note about file formats**

There are many different types of spatial [file storage formats](https://geocompr.robinlovelace.net/read-write.html?q=file#file-formats).

If you work a lot in ArcGIS, you'll probably want to stick with shapefiles (.shp) for vector data. But if you think you can get away with doing everything in R (hopefully after this workshop you will!), I highly recommend using geopackages. They are more compressed and only have one file (shapefiles tend to have \~5), so they're easier to manage and send to collaborators. They also don't place character length limits on attribute names.

It's easy to switch between file formats. Let save our electoral boundaries as a geopackage, and then read it back in.

```{r}
st_write(elec, 'data/E_AUGEC_region.gpkg', append = F)
elec <- st_read('data/E_AUGEC_region.gpkg')
```

#### Exploring with quick, interactive maps

One of the most important things to do when first reading in and checking your spatial data is to map it, ideally interactively so you can zoom in and out.

These days there are lots of options for plotting spatial data in R (i.e., mapping the data). You can use base R or `ggplot`, one of the best graphics-making packages out there. There's also `mapview`, `leaflet`, and many others.

My go-to is `tmap`. You can make interactive maps to explore your data very quickly, with just one line of code. You can also make beautiful, publication-quality maps. We'll try both.

First, let's do a 'quick thematic map' using the 'qtm' function from `tmap`.

```{r}
library(tmap)
tmap_mode('view')
qtm(elec, basemap = "Esri.WorldTopoMap")
```

Looks good! Try clicking on when of the site locations and notice the pop-up windows. In other interactive mapping packages (e.g., `leaflet`) we need to write lots of extra code to get this kind of functionality. Here only 3 lines of code!!

Today we're going to focus on the environment around the USC Moreton Bay Campus, so let's use some dplyr to filter for just this electorate: Dickson. This kind of boundary file is useful to have when you're honing down to a particular project area.

```{r}
library(dplyr)
dickson <- elec |> 
  filter(Elect_div == "Dickson")
```

**Note** You can use a *pipe* (`|>`) to link a dataframe to multiple functions (in this case just to filter). Piping can make your code easier to read, and more efficient to write by taking up less space.

Let's read in some other site-relevant vector data and map it all together.

```{r}
library(readr)
echidna_raw <- read_csv('data/echidna.csv')
rbvg_raw <- st_read('data/rbvg.gpkg')
```

You will notice that our echidna data is not a spatial data file, it's a csv! It also has a lot of variables. Data from these kinds of repositories are often messy. Let's tidy that up to keep just what is useful to us here.

```{r}
echidna_tidy <- echidna_raw |> select('basisOfRecord',
                                 'eventDate',
                                 'samplingProtocol',
                                 'locality',
                                 'decimalLatitude',
                                 'decimalLongitude',	
                                 'geodeticDatum',
                                 'coordinateUncertaintyInMeters',
                                 'vernacularName',
                                 'species')
```                            

Okay, our data is tidied, let's turn this dataframe into a spatial dataframe with st_as_sf. You will notice that our csv had decimalLatitude and decimalLongitude, we can use these as our coordinates. The ALA uses WGS84 as its CRS, which has the EPSG shortcode 4326.

```{r}
echidna_tidy <- echidna_tidy |> 
st_as_sf(coords=c("decimalLongitude","decimalLatitude"),crs=4326)
```

Let's hone our points down to only have echidna sightings within our area of interest.
We can use the 'st_intersection' function from `sf`. 

```{r}
echidna <- st_intersection(echidna_tidy, dickson)
```
Ah-ha! Our first error. Unfortunately, errors are very, very common when doing things in R. But the more errors you encounter, the better you'll get at trouble-shooting them. 80% of coding is googling error messages I reckon.

To get good at trouble-shooting errors, read the error message and try to infer what it is telling you. Some error messages aren't so hard to guess the problem, but others are much more cryptic. That's when googling comes in handy. I've found that trying over and over again to decipher error messages has paid off; it's gotten easier over time, and I don't have to ask google as much.

This error message tells us that the coordinate reference systems (crs's) of our echidna and elec data are not equal, i.e., are not the same. Our spatial data needs to be in the same projected crs to process and map together.

With `sf` we can easily project our echidna data to be in the same crs as our elec data using the function st_transform. Notice we use the function st_crs to get and check the coordinate reference system of our electorate data.

```{r}
st_crs(echidna_tidy)
st_crs(dickson)
st_crs(rbvg_raw)
```

It looks like our rbvg data is in the same CRS as our electorate data, we will just need to transform the echidna data before cropping.

```{r}
st_crs(dickson)
echidna_tidy <- st_transform(echidna_tidy, st_crs(dickson))
rbvg <- st_transform(rbvg_raw, st_crs(dickson))
echidna <- st_intersection(echidna_tidy, dickson) |> 
  mutate(ID = row_number()) # ID so we can more easily compare sites later
rbvg_crop <- st_intersection(rbvg_raw, dickson)
```

Now let's do a quick interactive map of all data layers.

```{r}
qtm(dickson) + qtm(rbvg_crop, 'bvg1m') + qtm(echidna)
```

We'll learn more about how to make a static, publication-quality map with `tmap` later.

For now you will notice that the colour of the rbvg looks a little off, and if you zoom in close enough, there are lines across some sections of the map. The first issue is flagged in the warning that qtm gave us, that we have too many variables to match the maximum for a fill colouring. The second is a rendering bug that occurs with interactive leaflet maps that have over 500 polygons.

We can fix this by merging some of the same polygons together. For our purposes, we don't need individual non-remnant patches You can combine many polygons into one polygon with the st_union function. However, we want to combine ours by categories. Fortunately dplyr and sf continue to work in harmony here. If we use group_by and summarize, it will use st_union by group!

We can also resolve the colour issue by using a custom colour palette for 36 colours using the Polychrome package. Realistically, we have too many categories, but this will do in a pinch.

```{r}
rbvg <- rbvg_crop |>
  group_by(bvg1m) |> 
  summarize() 

library(Polychrome)
P36 = unname(createPalette(36,  c("#ff0000", "#00ff00", "#0000ff")))

veg_map <- qtm(rbvg, col=NULL, 'bvg1m', fill.scale = tm_scale_categorical(n.max = 36, values = P36))
```

You will notice that it looks a bit blocky when zoomed out. This is caused by polygon simplification, which is not the case when you zoom in.


#### Buffering, spatial joins, and intersections

Perhaps we would like to count the number of different vegetation types within a 500m radius of each echidna sighting. First we'll create the buffers using `sf`s 'st_buffer' function.

```{r}
echidna_buff <- st_buffer(echidna, 500)
veg_map + qtm(echidna_buff)
```

Looks good. Now we just need to 1) find out which vegetation polygons intersect with each of the echidna sighting buffers, and 2) count up the unique vegetation types for each site.

For the first step we'll do a spatial join to find out where vegetation polygons and echidna buffers intersect using the `sf` function 'st_join'.

```{r}
echidna_veg <- st_join(echidna_buff, rbvg)
dim(echidna_buff)
dim(rbvg)
dim(echidna_veg)
```

Notice that our 'joined' spatial dataframe is longer (more rows) and wider (more columns) than our individual echidna and veg dataframes. Let's take a look at what's happened.

```{r}
head(echidna_veg)
```

Our new joined dataframe has all of the attributes from the echidna and vegetation data combined. Note also the row numbers on the left-hand side of the dataframe - '1, 1.1, 1.2, etc.'. What's happened is that our sites have been duplicated where a new row has been made for every vegetation polygon that intersects with a echidna buffer.

To get a quick summary of our sightings, we can count up the number of unique vegetation types that intersect with each echidna sighting. We can do that like we would a normal dataframe in R, with `dplyr` package functions.

```{r}
echidna_veg_sum <- echidna_veg |> 
  group_by(ID) |> # group by echidna sighting
  summarise(veg_count = length(unique(bvg1m)), site_sum = sum(st_area(echidna_veg_sum))) # count the number of unique vegetation types
head(echidna_veg_sum)
```



If you want to visually verify that the the summarise above worked as expected, just do an intersection of the first echidna buffer (ID==1) with the vegetation data and map it. Based on the above, we expect to see 4 vegetation types.

```{r}
site1 <- filter(echidna_buff, ID == '1') 
site1_veg <- st_intersection(site1, rbvg)
qtm(site1_veg, 'bvg1m', basemap = "Esri.WorldTopoMap")
```

Perfect!

We can also use our spatial data to calculate the area of vegetation types that are found where our echidna sightings occur. 

```{r}
echidna_veg_sites <- st_intersection(echidna_buff, rbvg)

echidna_veg_sites$site_area <- st_area(echidna_veg_sites) #Take care of units

echidna_area <- echidna_veg_sites |> 
  group_by(bvg1m) |> 
  summarise(site_sum = sum(site_area))

head(echidna_area)
```

Here we have used some functions we previously explored to first cut out the intersections where the rbvg and echidna buffer overlap. We have used the st_area function to calculate area for the vegetation at each site. Then finally used a similar group_by and summarise process to calculate the total area for each vegetation type across all sites.

We can now take this data and visualise it as we please.

```{r}
library(ggplot2)
library(units)

echidna_sum |> 
  arrange(-site_sum) |> 
  ggplot(aes(x= bvg1m, y=site_sum)) +
  geom_col()
```

We need to use the units library to work with square metres.
We can then tidy the graph to better present the data.

```{r}
echidna_sum |> 
  arrange(-site_sum) |> 
  ggplot(aes(x= forcats::fct_reorder(bvg1m, desc(site_sum)), y=site_sum)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
    labs(title = "Area of Vegetation Types around Echidna Sightings",
         x = "Vegetation Type",
         y = "Total Area")
```


Now that you have learned to explore vector data in R, your next step is to learn to use raster data. You can find that in the original version of [this tutorial by Christina Buelow](https://r-spatial-intro.netlify.app/#using-terra-to-read-in-raster-data).


### Common problems in Rspatial

Something that happens often when using R for spatial data, is that we encounter geometry errors that other spatial software (e.g., QGIS, ArcGIS) tend to fix 'behind the scenes'. I find myself using this excellent post on [tidying feature geometries](https://r-spatial.org/r/2017/03/19/invalid.html) by Edzer Pebesma, a developer of spatial packages in R, to solve these kinds of problems.

Learning how to use R for spatial data science can be a steep learning curve. But, at least for me, the effort has really paid off in terms of the scale and types of spatial analyses I can run. For big, global analyses, I can run my R script on a high-performance computer (HPC) and speed it up with parallelisation. And everything is scripted and reproducible, making it easier for me to make my science open and transparent.

Having a group of colleagues and friends to discuss and solve common problems can make the learning curve more fun. Here in Brisbane you can join the [UQ geospatial community](https://geospatial-community.netlify.app/), and you can meet a broader community of programming enthusiast at [RLadies Brisbane](https://www.meetup.com/en-AU/rladies-brisbane/?_cookie-check=yPZvMC_FbOGrDFxo) events.

See below for more resources and communities that can also be incredibly helpful, including stack exchange and twitter.

Happy R'ing!!

### Useful resources

-   Google

-   [Geospatial Share Tutorials](https://brisbane-geocommunity.netlify.app/workshops)

-   [Stack Exchange](https://stackexchange.com/)

-   Twitter [#rspatial](https://twitter.com/ChristinABuelow/status/1547145678941736961)

-   [Geocomputation in R](https://geocompr.robinlovelace.net/)

-   [Spatial Data Science with applications in R](https://keen-swartz-3146c4.netlify.app/)

-   [Spatial Data Science with R and terra](https://rspatial.org/terra/index.html)

-   [R cheat sheets](https://www.rstudio.com/resources/cheatsheets/)

-   [Australian climate data](http://www.bom.gov.au/jsp/ncc/climate_averages/rainfall/index.jsp)
